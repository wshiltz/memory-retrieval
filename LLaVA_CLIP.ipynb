{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a63522-221f-43bf-9250-9016e1f0f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must use python 3.10 for this projcet. will also need to delete old conda envs in the morning\n",
    "#This cell is a hugging face sanity check\n",
    "#idea use clip to fectch t20 photos matching a user description use LLava to generate captions for 20 photos vector embeded them and compare to original prompt for similarity. \n",
    "#https://grok.com/share/c2hhcmQtMg%3D%3D_36b5ade9-4791-401a-86bf-3cf9eb29e0fa\n",
    "\n",
    "# import torch\n",
    "# import accelerate\n",
    "# from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "# ### This cell is bassically a sanity check to ensure conda works. \n",
    "\n",
    "# # Load the model in half-precision\n",
    "# model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=True)\n",
    "\n",
    "# conversation = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n",
    "#             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "#         ],\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# inputs = processor.apply_chat_template(\n",
    "#     conversation,\n",
    "#     add_generation_prompt=True,\n",
    "#     tokenize=True,\n",
    "#     return_dict=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device, torch.float16)\n",
    "# # Generate\n",
    "# generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "# processor.batch_decode(generate_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aa31d-50b0-45ea-8776-96ea6f040e88",
   "metadata": {},
   "source": [
    "# CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff0ec76-0c7c-4aca-94b5-ce7a72978cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MemoryRetrieval import *\n",
    "# getting image paths\n",
    "import os\n",
    "## Load LLava model\n",
    "LLava_image_dir = \"T20photos\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d2c3eb-c19d-4e60-bc41-a972e3560399",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f17ab7b471c4f2ea7981d7d1c5f8026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Model setup\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16).to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=True)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define the base directory where your images are stored\n",
    "base_dir = './smallDataset/'\n",
    "image_dir = \"testing_photos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c90fee9b-5d18-4109-ae06-2a15e4ef051c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Pre compute clip embeddings based on photo library so tht dir should be all the photos\n",
    "from PIL import Image \n",
    "\n",
    "def encode_images(image_dir):\n",
    "    image_embeddings = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for img_path in os.listdir(image_dir):\n",
    "        img = Image.open(os.path.join(image_dir, img_path))\n",
    "        img = clip_preprocess(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = clip_model.encode_image(img).cpu().numpy()\n",
    "        image_embeddings.append(emb)\n",
    "        image_paths.append(img_path)\n",
    "    \n",
    "    # Build Faiss index\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "    index.add(image_embeddings)\n",
    "    \n",
    "    return index, image_embeddings, image_paths\n",
    "\n",
    "# clip encodes images\n",
    "clip_index, clip_embeddings, image_paths = encode_images(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac009f-1bbb-43cf-bd0f-d3d57c7d876b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Facial Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd5d80e-bfc1-496f-92dc-76a580eb5c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def user_in_image(img_path):\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43846492-eb3b-48b8-8c37-3b2f3e035f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e301f-e848-4815-833c-bb22c685f0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d5fdc2-f45e-41a1-83b1-2325845c7f60",
   "metadata": {},
   "source": [
    "# LLava Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba367210-c825-4082-b15f-2e371064a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##test captioning and similarity\n",
    "image = Image.open(\"Sample_image.jpg\").convert(\"RGB\")\n",
    "print(caption_image(image, llava_model, processor))\n",
    "print(image_similarity(caption_image(image, llava_model, processor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a4ef2-6c63-4655-909e-91819eded01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a function that utilizes caption facial recognition and clip to score and output highest scoring image\n",
    "def rank_photos(prompt, n=10, prio_user=True, user_weight):\n",
    "    #retrieve initial batch of photos with CLIP\n",
    "    prompt_emb = clip_model.encode_text(clip.tokenize([prompt]).to(device)).cpu().numpy()\n",
    "    distances, indices = clip_index.search(prompt_emb, n*2)\n",
    "    clip_scores = 1 - distances[0] / np.max(distances[0]) #normalize to be between 0-1\n",
    "    # LLava Captions photos and compuites a similarity score.\n",
    "    llava_scores = []\n",
    "    captions = []\n",
    "    for idx in indicies[0]:\n",
    "        img_path = os.path.join(image_dir, image_paths[idx])\n",
    "        caption = get_llava_caption(img_path)\n",
    "        score = caption_similarity(prompt, caption)\n",
    "        llava_scores.append(score)\n",
    "        captions.append(caption)\n",
    "    llava_scores = np.array(llava_scores)\n",
    "    llava_scores = llava_scores / np.max(llava_scores)  # Normalize\n",
    "    \n",
    "    # use facial recognition to computre probability of a user being in the photo\n",
    "    user_present = [] #score of if user is present in photo\n",
    "    for idx in indices[0]:\n",
    "        img_path = os.path.join(image_dir, image_paths[idx])\n",
    "        score = user_in_image(img_path) if prio_user else 0.0 #change this to whatever your function to calculate user dection is\n",
    "        user_present.append(score)\n",
    "    user_present = np.array(user_present)\n",
    "    #score each image and output n images in descending order.\n",
    "    image_scores = 0.4 * clip_scores + 0.2 * llava_scores + user_weight * me_scores\n",
    "    top_n_images = np.argsort(final_scores)[::-1][:n]\n",
    "    \n",
    "    results = []\n",
    "    for i in top_n_images:\n",
    "        idx = indices[0][i]\n",
    "        results.append({\n",
    "            \"path\": image_paths[idx],\n",
    "            \"caption\": captions[i],\n",
    "            \"clip_score\": clip_scores[i],\n",
    "            \"llava_score\": llava_scores[i],\n",
    "            \"me_score\": me_scores[i],\n",
    "            \"final_score\": final_scores[i]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d88f6-435e-4e1a-b6ce-2c2d8cec2ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae8351-760a-4b77-b05c-b26522e4fa33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69ddb9-9ca8-4f35-aa81-0584c0bfc890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLava_env",
   "language": "python",
   "name": "llava_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
